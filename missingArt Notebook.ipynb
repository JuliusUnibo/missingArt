{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bda5c4d",
   "metadata": {},
   "source": [
    "With this python-script I prepared the orignal csv-data for further usage and identified the affected artists via the Wikidata Q-Number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import urllib.error\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Clean texts\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'[()]', '', text)\n",
    "    text = re.sub(r'[\\d\\.]+', '', text) # Deletes numbers and points\n",
    "    text = re.sub(r'/', '', text)  # Deletes slashes\n",
    "    text = re.sub(r'\\|.*', '', text)  # Deletes \"|\"\n",
    "    text = re.sub(r'\\'.*', '', text) # Deletes \"'\"\n",
    "\n",
    "    return text\n",
    "\n",
    "# Clean dates\n",
    "def clean_date(date):\n",
    "    if date is not None:\n",
    "        date = re.sub(r'\\[.*?\\]', '', date)\n",
    "        date = re.sub(r'\\s+', ' ', date).strip()\n",
    "        date = re.sub(r'[()]', '', date)    \n",
    "        date = re.sub(r'[/\\-].*', '', date) # Delete everything after a slash and the slash\n",
    "        if re.match(r'^\\d{4}$', date):\n",
    "            return date\n",
    "        if '/' in date:\n",
    "            date = date.split('/')[0] + \" (?)\"\n",
    "        if any(term in date for term in [\"ca.\", \"um\", \"nach\", \"?\"]):\n",
    "            date = re.sub(r\"ca\\.|um|nach|\\?\", \"\", date).strip() + \" (?)\"\n",
    "    else:\n",
    "        date = \"unknown\"\n",
    "    return date\n",
    "\n",
    "# Function to fill empty cells with \"null\"\n",
    "def fill_empty_cells(rows, column_names):\n",
    "    for row in rows:\n",
    "        for column_name in column_names:\n",
    "            if column_name in row and not row[column_name].strip():\n",
    "                row[column_name] = \"null\"\n",
    "    return rows\n",
    "\n",
    "def query_wikidata(label, lang, sparql):\n",
    "    query = f\"\"\"\n",
    "    SELECT ?person WHERE {{\n",
    "      ?person rdfs:label \"{label}\"@{lang}.\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],{lang}\". }}\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()\n",
    "\n",
    "# Cache-System because of frequent time out errors\n",
    "q_number_cache = {}\n",
    "\n",
    "# SPARQL-Query to get Q-Number of artists in dataset\n",
    "def get_wikidata_q_number(label):\n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "    \n",
    "    if label in q_number_cache:\n",
    "        return q_number_cache[label]\n",
    "    \n",
    "    for lang in [\"en\", \"it\", \"de\"]:\n",
    "        for _ in range(5):\n",
    "            try:\n",
    "                results = query_wikidata(label, lang, sparql)\n",
    "                if results[\"results\"][\"bindings\"]:\n",
    "                    q_number = results[\"results\"][\"bindings\"][0][\"person\"][\"value\"].split('/')[-1]\n",
    "                    q_number_cache[label] = q_number\n",
    "                    return q_number\n",
    "                else:\n",
    "                    query_native = f\"\"\"\n",
    "                    SELECT ?person WHERE {{\n",
    "                      ?person wdt:P1559 \"{label}\"@{lang}.\n",
    "                      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],{lang}\". }}\n",
    "                    }}\n",
    "                    LIMIT 1\n",
    "                    \"\"\"\n",
    "                    sparql.setQuery(query_native)\n",
    "                    results_native = sparql.query().convert()\n",
    "                    if results_native[\"results\"][\"bindings\"]:\n",
    "                        q_number = results_native[\"results\"][\"bindings\"][0][\"person\"][\"value\"].split('/')[-1]\n",
    "                        q_number_cache[label] = q_number\n",
    "                        return q_number\n",
    "                    else:\n",
    "                        label_temp = re.sub(r\" the Younger| the Elder\", \"\", label).strip()\n",
    "                        query_temp = f\"\"\"\n",
    "                        SELECT ?person WHERE {{\n",
    "                          ?person rdfs:label \"{label_temp}\"@{lang}.\n",
    "                          SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],{lang}\". }}\n",
    "                        }}\n",
    "                        LIMIT 1\n",
    "                        \"\"\"\n",
    "                        sparql.setQuery(query_temp)\n",
    "                        results_temp = sparql.query().convert()\n",
    "                        if results_temp[\"results\"][\"bindings\"]:\n",
    "                            q_number = results_temp[\"results\"][\"bindings\"][0][\"person\"][\"value\"].split('/')[-1]\n",
    "                            q_number_cache[label] = q_number\n",
    "                            return q_number\n",
    "                        else:\n",
    "                            continue\n",
    "            except urllib.error.HTTPError as e:\n",
    "                if e.code == 429:\n",
    "                    time.sleep(3)\n",
    "                    print(\"Musste es erneut versuchen.\")\n",
    "                elif e.code == 403:\n",
    "                    print(\"HTTP Error 403: Forbidden. Überprüfen Sie die Authentifizierung und Autorisierung.\")\n",
    "                    return \"null\"\n",
    "                else:\n",
    "                    raise e\n",
    "    \n",
    "    q_number_cache[label] = \"null\"\n",
    "    return \"null\"\n",
    "\n",
    "def process_row(row):\n",
    "    full_name = row.get('Hersteller/Künstler/Autor:in', '')\n",
    "    date = row.get('Datierung', '')\n",
    "    \n",
    "    # Check if the name is not none\n",
    "    if full_name and (\"Unbekannt\" in full_name or \"Schule\" in full_name or \"Werkstatt\" in full_name or \"Meister\" in full_name):\n",
    "        return None\n",
    "    \n",
    "    row['namen_neu'] = \"\"\n",
    "    row['Q_Nummer'] = \"\"\n",
    "    row['datum_neu'] = \"\"\n",
    "\n",
    "    # Check uncertainties of the artists and respect them in the clean data     \n",
    "    name_uncertain = False\n",
    "    \n",
    "    if full_name is not None:\n",
    "        name_uncertain = any(term in full_name for term in [\"möglicherweise\", \"zugeschrieben\", \"atttributed to\", \"nach\", \"Vermutlich\", \"vermutlich\", \"im Stil von\", \"Stil von\",\"Kreis des\", \"Kreis von\", \"Manner of\", \"Art des\", \"Kopie\", \"?\"])\n",
    "    if name_uncertain:\n",
    "        full_name = re.sub(r\"möglicherweise|zugeschrieben|attributed to|nach|Vermutlich|vermutlich|im Stil von|Stil von|Kreis des|Kreis von|Manner of|Art des|Kopie|\\?\", \"\", full_name).strip()\n",
    "        full_name = re.sub(r'\\s+', ' ', full_name).strip()\n",
    "    else:\n",
    "        name_uncertain = False\n",
    "    \n",
    "    date_uncertain = False\n",
    "\n",
    "    if date is not None:\n",
    "        date_uncertain = any(term in date for term in [\"ca.\", \"um\", \"(?)\"]) or '/' in date\n",
    "    else:\n",
    "        date_uncertain = False\n",
    "\n",
    "\n",
    "    if full_name is not None:\n",
    "        if full_name.count(',') == 1:\n",
    "            family_name, given_name = full_name.split(', ')\n",
    "            cleaned_family_name = clean_text(family_name)\n",
    "            cleaned_given_name = clean_text(given_name)\n",
    "            \n",
    "            new_name = f\"{cleaned_given_name} {cleaned_family_name}\"\n",
    "            \n",
    "            # Translate or convert spelling of \"the Elder\" or \"the Younger\"\n",
    "            if \"der Jüngere\" in new_name or \"d J\" in new_name:\n",
    "                new_name = re.sub(r\"der Jüngere|d J\", \"\", new_name).strip() + \" the Younger\"\n",
    "            \n",
    "            if \"der Ältere\" in new_name or \"d Ä\" in new_name:\n",
    "                new_name = re.sub(r\"der Ältere|d Ä\", \"\", new_name).strip() + \" the Elder\"\n",
    "            \n",
    "            new_name = re.sub(r'\\s+', ' ', new_name).strip()\n",
    "            \n",
    "            row['namen_neu'] = new_name + \" (?)\" if name_uncertain else new_name\n",
    "            \n",
    "            q_number = get_wikidata_q_number(new_name)\n",
    "            row['Q_Nummer'] = q_number\n",
    "\n",
    "        \n",
    "        elif ',' not in full_name:\n",
    "            cleaned_name = clean_text(full_name)\n",
    "            row['namen_neu'] = cleaned_name + \" (?)\" if name_uncertain else cleaned_name\n",
    "            q_number = get_wikidata_q_number(cleaned_name)\n",
    "            row['Q_Nummer'] = q_number\n",
    "    else:\n",
    "        print(\"full_name ist None.\")\n",
    "        \n",
    "        row['namen_neu'] = \"unknown\"\n",
    "        row['Q_Nummer'] = \"unknown\"\n",
    "    \n",
    "    cleaned_date = clean_date(date)\n",
    "    row['datum_neu'] = cleaned_date\n",
    "        \n",
    "    print(f\"Original: {row.get('Hersteller/Künstler/Autor:in', '')} - Bereinigt: {row['namen_neu']}\")\n",
    "    print(f\"Originaldatum: {row.get('Datierung', '')} - Bereinigt: {row['datum_neu']}\")\n",
    "\n",
    "    return row\n",
    "\n",
    "with open('missingArtMid.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file, delimiter=';')\n",
    "    rows = list(reader)\n",
    "\n",
    "# Delete columns, which the project does not need\n",
    "columns_to_remove = [\"Literatur / Quelle\", \"Link\", \"Kontakt\", \"Veröffentlicht seit\", \"Provenienz\", \"Inventarnummer/Signatur\", \"Objektart\", \"Datensatzart\", \"Meldungsart\"]\n",
    "for row in rows:\n",
    "    for col in columns_to_remove:\n",
    "        if col in row:\n",
    "            del row[col]\n",
    "\n",
    "fieldnames = list(rows[0].keys()) + ['namen_neu', 'Q_Nummer', 'datum_neu']\n",
    "\n",
    "filtered_rows = []\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results = list(executor.map(process_row, rows))\n",
    "\n",
    "filtered_rows = [row for row in results if row is not None]\n",
    "\n",
    "# Fill up empty columns\n",
    "columns_to_fill = ['datum_neu', 'Q_Nummer', 'namen_neu']\n",
    "filtered_rows = fill_empty_cells(filtered_rows, columns_to_fill)\n",
    "\n",
    "with open('missingArtMid.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames, delimiter=';')\n",
    "    writer.writeheader()\n",
    "    writer.writerows(filtered_rows)\n",
    "\n",
    "print(\"Alles bereinigt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44404e11",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "After that I used different Python-Scripts for each type of visualization. The first one splitted the dates from the artworks in order to differenciate between certain and uncertain dates. The script also added a new column with the value \"1\" in order to visualize the data on a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e029464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the prepared data\n",
    "df = pd.read_csv('missingArtMid.csv', delimiter=';')\n",
    "\n",
    "# Create new column with default value \"0\" to differenciate the dates\n",
    "df['datum_alt'] = '0'\n",
    "\n",
    "# Function to clean the \"datum_neu\" column\n",
    "def clean_datum_neu(value):\n",
    "    if pd.isna(value):\n",
    "        return '0', '0'\n",
    "    elif isinstance(value, str) and ' (?)' in value:\n",
    "        return '0', value\n",
    "    elif isinstance(value, str) and value.isdigit() and len(value) == 4:\n",
    "        return value, '0'\n",
    "    else:\n",
    "        return value, '0'\n",
    "\n",
    "# Apply the function to each row in the \"datum_neu\" column\n",
    "df['datum_neu'], df['datum_alt'] = zip(*df['datum_neu'].apply(clean_datum_neu))\n",
    "\n",
    "# Remove \" (?)\" from \"datum_alt\" column\n",
    "df['datum_alt'] = df['datum_alt'].str.replace(' (?)', '', regex=False)\n",
    "\n",
    "# Function to check if a value contains exactly four digits\n",
    "def is_four_digits(value):\n",
    "    return isinstance(value, str) and value.isdigit() and len(value) == 4\n",
    "\n",
    "# Add a new column \"spalte\" with default value \"1\"\n",
    "df['spalte'] = 1\n",
    "\n",
    "# Save the prepared data\n",
    "df.to_csv('missingArt0.csv', index=False, sep=';')\n",
    "print(\"Die Datei wurde erfolgreich bereinigt und als 'missingArt0.csv' gespeichert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c168ee67",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "The next script adds values of latitude and longitude to the data in order to visualize them on a map. The values are gathered via a SPARQL-Query from the earlier identified Wikidata Q-Numbers of the artists and their respective properties \"work location\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31889444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "# Load the prepared data\n",
    "df = pd.read_csv('missingArtMid.csv', delimiter=';')\n",
    "\n",
    "# Add new columns with default values (coordinates of Berlin as starting point of each graph)\n",
    "df['lat0'] = 52.516667\n",
    "df['long0'] = 13.383333\n",
    "\n",
    "for i in range(1, 6):\n",
    "    df[f'lat{i}'] = 'null'\n",
    "    df[f'long{i}'] = 'null'\n",
    "\n",
    "# Function to query Wikidata for work locations\n",
    "def query_wikidata(q_number):\n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "    sparql.setQuery(f\"\"\"\n",
    "    SELECT ?coordinate WHERE {{\n",
    "      wd:{q_number} wdt:P937 ?location .\n",
    "      ?location wdt:P625 ?coordinate .\n",
    "    }}\n",
    "    \"\"\")\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    sparql.addCustomHttpHeader(\"User-Agent\", \"MyUserAgent/1.0\")\n",
    "    results = sparql.query().convert()\n",
    "    \n",
    "    coordinates = []\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        coord = result[\"coordinate\"][\"value\"].strip('Point()').split()\n",
    "        coordinates.append((float(coord[1]), float(coord[0])))\n",
    "    \n",
    "    return coordinates\n",
    "\n",
    "# Put the coordinates from Wikidata in the data set\n",
    "for index, row in df.iterrows():\n",
    "    q_number = row['Q_Nummer']\n",
    "    coordinates = query_wikidata(q_number)\n",
    "    \n",
    "    for i, (lat, long) in enumerate(coordinates[:5], start=1):\n",
    "        df.at[index, f'lat{i}'] = lat\n",
    "        df.at[index, f'long{i}'] = long\n",
    "\n",
    "# Delete some other columns\n",
    "df.drop(columns=['datum_neu', 'Titel'], inplace=True)\n",
    "\n",
    "# Save the new data\n",
    "df.to_csv('missingArt1.csv', index=False, sep=';')\n",
    "print(\"Hat alles geklappt. Ich habe die Änderungen in der neuen Datei missingArt1.csv gespeichert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556e8b29",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Then I used this straightforward script to identify unique names in the data set and count them. With this information I can visualize the most affected artists in a lollipop chart.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c0dec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the prepared data\n",
    "df = pd.read_csv('missingArtMid.csv', delimiter=';')\n",
    "\n",
    "# Count the same values in one column\n",
    "name_counts = df['namen_neu'].value_counts()\n",
    "\n",
    "# Create a data set for unique names\n",
    "unique_names_df = name_counts.reset_index()\n",
    "unique_names_df.columns = ['namen_neu', 'anzahl_artworks']\n",
    "\n",
    "# Implement the counted unique names in the data set \n",
    "df = df.drop_duplicates(subset=['namen_neu'])\n",
    "df = df.merge(unique_names_df, on='namen_neu', how='left')\n",
    "\n",
    "# Save the data set\n",
    "df.to_csv('missingArt2.csv', index=False, sep=';')\n",
    "\n",
    "print(\"Die Datei wurde erfolgreich bereinigt und als 'missingArt2.csv' gespeichert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2c1294",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "The following script tries to identify the art style of each painter via a SPARQL-Query. I took only into account art styles which I could identify via the movement from the artist or from one of his notable works featured in the Wikidata data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f228048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import concurrent.futures\n",
    "\n",
    "# Load the prepared data\n",
    "df = pd.read_csv('missingArtMid.csv', sep=';')\n",
    "\n",
    "# Delete again some columns\n",
    "df.drop(columns=[\"Lost Art ID\", \"Hersteller/Künstler/Autor:in\", \"Datierung\", \"Beschreibung\"], inplace=True)\n",
    "\n",
    "# Identify unique artists\n",
    "counts = df['namen_neu'].value_counts()\n",
    "\n",
    "df = df.drop_duplicates(subset=['namen_neu'])\n",
    "df['anzahl_artworks'] = df['namen_neu'].map(counts)\n",
    "\n",
    "# SPARQL-Query to obtain the property movement of the artist or one of his/her notable works\n",
    "def get_genre(q_number):\n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT ?movementLabel WHERE {{\n",
    "      wd:{q_number} wdt:P135 ?movement .\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    sparql.addCustomHttpHeader(\"User-Agent\", \"MyUserAgent/1.0\")\n",
    "    results = sparql.query().convert()\n",
    "    \n",
    "    if results[\"results\"][\"bindings\"]:\n",
    "        return results[\"results\"][\"bindings\"][0][\"movementLabel\"][\"value\"]\n",
    "    \n",
    "    # Only applies if no property was obtained earlier\n",
    "    query = f\"\"\"\n",
    "    SELECT ?notableWork WHERE {{\n",
    "      wd:{q_number} wdt:P800 ?notableWork .\n",
    "    }}\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    \n",
    "    if results[\"results\"][\"bindings\"]:\n",
    "        notable_work = results[\"results\"][\"bindings\"][0][\"notableWork\"][\"value\"].split('/')[-1]\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT ?movementLabel WHERE {{\n",
    "          wd:{notable_work} wdt:P135 ?movement .\n",
    "          SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        sparql.setQuery(query)\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        results = sparql.query().convert()\n",
    "        \n",
    "        if results[\"results\"][\"bindings\"]:\n",
    "            return results[\"results\"][\"bindings\"][0][\"movementLabel\"][\"value\"]\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Function to do queries at the same time\n",
    "def parallel_get_genre(q_numbers):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(get_genre, q_numbers))\n",
    "    return results\n",
    "\n",
    "df['genre'] = parallel_get_genre(df['Q_Nummer'])\n",
    "\n",
    "# Group the data via the art style\n",
    "df = df.groupby('genre').agg({\n",
    "    'anzahl_artworks': 'sum',\n",
    "    'namen_neu': lambda x: ', '.join(x)\n",
    "}).reset_index()\n",
    "\n",
    "# Save the data set\n",
    "df.to_csv('missingArt3.csv', sep=';', index=False)\n",
    "\n",
    "print(\"Die CSV-Datei wurde bereinigt und als 'missingArt3.csv' gespeichert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0fcb45",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "For the last visualization I used the europeana search API to gather information about online repositories that provide online access to artworks from the affected painters. I had to use a JSON-Query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43184df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# API-Key\n",
    "api_key = 'PLACEHOLDER' # Subsitute with API-Key from europeana\n",
    "\n",
    "# API-Endpoint from europeana search, in this case json und not SPARQL\n",
    "search_endpoint = 'https://www.europeana.eu/api/v2/search.json'\n",
    "\n",
    "# Load prepared data\n",
    "df = pd.read_csv('missingArtMid.csv', sep=';')\n",
    "\n",
    "# Clean the names to get better results\n",
    "artists = df['namen_neu'].str.replace(r'\\s*\\(\\?\\)', '', regex=True)\n",
    "\n",
    "# Query to count the search results for each artist and obtain the top six results\n",
    "def search_artist(artist_name):\n",
    "    params = {\n",
    "        'query': f'proxy_dc_creator:\"{artist_name}\"',\n",
    "        'wskey': api_key,\n",
    "        'rows': 100 \n",
    "    }\n",
    "    response = requests.get(search_endpoint, params=params)\n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "        domains = []\n",
    "        for item in results['items']:\n",
    "            is_shown_at = item.get('edmIsShownAt', [])\n",
    "            if is_shown_at:\n",
    "                for url in is_shown_at:\n",
    "                    parsed_url = urlparse(url)\n",
    "                    domain = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "                    domains.append(domain)\n",
    "        domain_counts = Counter(domains)\n",
    "        top_domains = domain_counts.most_common(5)\n",
    "        return top_domains\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Apply on every artist\n",
    "all_domains = []\n",
    "for artist in artists:\n",
    "    top_domains = search_artist(artist)\n",
    "    all_domains.extend([domain for domain, count in top_domains])\n",
    "\n",
    "overall_domain_counts = Counter(all_domains)\n",
    "top_overall_domains = overall_domain_counts.most_common(6)  # Limit to the top six\n",
    "\n",
    "for domain, count in top_overall_domains:\n",
    "    print(f\"Domain: {domain}, Count: {count}\")\n",
    "\n",
    "# Save the data\n",
    "output_df = pd.DataFrame(top_overall_domains, columns=['domain', 'count'])\n",
    "output_df.to_csv('missingArt4.csv', index=False, sep=';')\n",
    "\n",
    "print(\"Die Ergebnisse wurden in 'missingArt4.csv' gespeichert.\")\n",
    "\n",
    "# Clean the earlier obtained URL for the label of the gallery\n",
    "def clean_url(url):\n",
    "    # Delete 'https://' oder 'http://'\n",
    "    url = re.sub(r'^https?://', '', url)\n",
    "    # Delete 'www.'\n",
    "    url = re.sub(r'^www\\.', '', url)\n",
    "    # Delete Top-Level-Domain\n",
    "    url = re.sub(r'\\.[a-z]{2,3}$', '', url)\n",
    "    return url\n",
    "\n",
    "df_cleaned['label_domain'] = df_cleaned['domain'].apply(clean_url)\n",
    "\n",
    "# Save the cleaned URL\n",
    "df_cleaned.to_csv('bereinigte_datei.csv', index=False, sep=';')\n",
    "\n",
    "print(\"Die bereinigte Datei wurde in 'bereinigte_datei.csv' gespeichert.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
